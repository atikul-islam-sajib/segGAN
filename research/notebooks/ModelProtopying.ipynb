{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import yaml\n",
    "import torch\n",
    "import joblib\n",
    "import zipfile\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "from torchview import draw_graph\n",
    "from torchvision import transforms\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config():\n",
    "    with open(\"../../config.yml\", \"r\") as file:\n",
    "        config_files = yaml.safe_load(file)\n",
    "        \n",
    "    return config_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathException(Exception):\n",
    "    def __init__(self, message):\n",
    "        super(PathException, self).__init__(message)\n",
    "        self.message = message\n",
    "\n",
    "def validate_path(path):\n",
    "    if os.path.exists(path):\n",
    "        return path\n",
    "    else:\n",
    "        traceback.print_exc()\n",
    "        raise PathException(\"{} Path does not exist\".capitalize().format(path))\n",
    "\n",
    "\n",
    "def dump(value=None, filename=None):\n",
    "    if (value is not None) and (filename is not None):\n",
    "        joblib.dump(value, filename)\n",
    "\n",
    "def load(filename=None):\n",
    "    if filename is not None:\n",
    "        return joblib.load(filename)\n",
    "\n",
    "\n",
    "def device_init(device=\"cuda\"):\n",
    "    if device == \"mps\":\n",
    "        return torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "    elif device == \"cuda\":\n",
    "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def weight_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader():\n",
    "    def __init__(self, image_path = None, channels = 3, image_size = 256, batch_size = 1, split_size = 0.20):\n",
    "        self.image_path = image_path\n",
    "        self.channels = channels\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.split_size = split_size\n",
    "\n",
    "        self.images = []\n",
    "        self.masks = []\n",
    "\n",
    "        self.config = config()\n",
    "\n",
    "        self.raw_path = validate_path(self.config[\"path\"][\"raw_path\"])\n",
    "        self.processed_path = validate_path(self.config[\"path\"][\"processed_path\"])\n",
    "        self.files_path = validate_path(self.config[\"path\"][\"files_path\"])\n",
    "\n",
    "    def transforms(self):\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((self.image_size, self.image_size), Image.BICUBIC),\n",
    "            transforms.CenterCrop((self.image_size, self.image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "    def unzip_folder(self):\n",
    "        with zipfile.ZipFile(self.image_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(os.path.join(self.raw_path,))\n",
    "\n",
    "        print(\"The unzip folder of image saved in {}\".format(self.raw_path))\n",
    "\n",
    "    def split_dataset(self, X, y):\n",
    "        if isinstance(X, list) and isinstance(y, list):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=self.split_size, random_state=42)\n",
    "            return {\n",
    "                \"X_train\": X_train,\n",
    "                \"X_test\": X_test,\n",
    "                \"y_train\": y_train,\n",
    "                \"y_test\": y_test\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"X and y must be of type list\".capitalize())\n",
    "\n",
    "    def feature_extractor(self):\n",
    "        self.directory = os.path.join(self.raw_path, \"datasets\")\n",
    "        self.images_path = os.path.join(self.directory, \"images\")\n",
    "        self.masks_path = os.path.join(self.directory, \"masks\")\n",
    "\n",
    "        self.masks_list = os.listdir(self.masks_path)\n",
    "\n",
    "        for index, image in tqdm(enumerate(os.listdir(self.images_path))):\n",
    "            image_name, _ = image.split(\".\")\n",
    "            mask_name, _ = self.masks_list[index].split(\".\")\n",
    "\n",
    "            if image_name == mask_name:\n",
    "                image_path = os.path.join(self.images_path, image)\n",
    "                mask_path = os.path.join(self.masks_path, self.masks_list[index])\n",
    "\n",
    "                image_X = cv2.imread(image_path)\n",
    "                image_Y = cv2.imread(mask_path)\n",
    "\n",
    "                image_X = cv2.cvtColor(image_X, cv2.COLOR_BGR2RGB)\n",
    "                image_Y = cv2.cvtColor(image_Y, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                image_X = Image.fromarray(image_X)\n",
    "                image_Y = Image.fromarray(image_Y)\n",
    "\n",
    "                image_X = self.transforms()(image_X)\n",
    "                image_Y = self.transforms()(image_Y)\n",
    "\n",
    "                self.images.append(image_X)\n",
    "                self.masks.append(image_Y)\n",
    "\n",
    "        return self.split_dataset(X=self.images, y=self.masks)\n",
    "\n",
    "    def create_dataloader(self):\n",
    "        data = self.feature_extractor()\n",
    "\n",
    "        self.train_dataloader = DataLoader(\n",
    "            dataset=list(zip(data[\"X_train\"],data[\"y_train\"])),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True\n",
    "        \n",
    "        )\n",
    "\n",
    "        self.test_dataloader = DataLoader(\n",
    "            dataset=list(zip(data[\"X_test\"], data[\"y_test\"])),\n",
    "            batch_size=self.batch_size * 8,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "        for filename, value in tqdm([(\"train_dataloader\", self.train_dataloader), (\"test_dataloader\", self.test_dataloader)]):\n",
    "            dump(\n",
    "                value=value,\n",
    "                filename=os.path.join(self.processed_path, \"{}.pkl\".format(filename))\n",
    "            )\n",
    "\n",
    "        print(\"all the dataloaders are saved in # {}\".format(self.processed_path))\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_images():\n",
    "        config_files = config()\n",
    "\n",
    "        processed_path = validate_path(path=config_files[\"path\"][\"processed_path\"])\n",
    "        files_path = validate_path(path=config_files[\"path\"][\"files_path\"])\n",
    "\n",
    "        test_dataloader = load(filename=os.path.join(processed_path, \"test_dataloader.pkl\"))\n",
    "\n",
    "        image_X, image_Y = next(iter(test_dataloader))\n",
    "\n",
    "        plt.figure(figsize=(20, 10))\n",
    "\n",
    "        for index, (image) in enumerate(image_X):\n",
    "            X = image.permute(1, 2, 0).cpu().detach().numpy()\n",
    "            Y = image_Y[index].permute(1, 2, 0).cpu().detach().numpy()\n",
    "\n",
    "            X = (X - X.min())/(X.max() - X.min())\n",
    "            Y = (Y - Y.min())/(Y.max() - Y.min())\n",
    "\n",
    "            for idx, (title, value) in enumerate([(\"X\", X), (\"Y\", Y)]):\n",
    "\n",
    "                plt.subplot(2 * 2, 2 * 4, 2 * index + idx + 1)\n",
    "                plt.imshow(value)\n",
    "                plt.title(title)\n",
    "                plt.axis(\"off\")\n",
    "\n",
    "        plt.savefig(os.path.join(files_path, \"image.jpeg\"))\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def dataset_details():\n",
    "        config_files = config()\n",
    "\n",
    "        processed_path = validate_path(path=config_files[\"path\"][\"processed_path\"])\n",
    "        files_path = validate_path(path=config_files[\"path\"][\"files_path\"])\n",
    "        \n",
    "        train_dataloader = load(filename=os.path.join(processed_path, \"train_dataloader.pkl\"))\n",
    "        test_dataloader = load(filename=os.path.join(processed_path, \"test_dataloader.pkl\"))\n",
    "        \n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"train_image(Total)\": str(sum(X.size(0) for X, _ in train_dataloader)),\n",
    "                \"test_image(Total)\": str(sum(X.size(0) for X, _ in test_dataloader)),\n",
    "                \"train_image(Batch)\": str(train_dataloader.batch_size),\n",
    "                \"test_image(Batch)\": str(test_dataloader.batch_size),\n",
    "                \n",
    "            },\n",
    "            index = [\"Quantity\"]\n",
    "        ).T.to_csv(os.path.join(files_path, \"dataset_details.csv\"))\n",
    "\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "    loader = Loader(image_path=\"/Users/shahmuhammadraditrahman/Desktop/datasets.zip\")\n",
    "\n",
    "    loader.unzip_folder()\n",
    "    loader.create_dataloader()\n",
    "    \n",
    "    loader.plot_images()\n",
    "    loader.dataset_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels = 3, out_channels = 64, use_leaky_relu = True, use_batch_norm = False):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.leaky_relu = use_leaky_relu\n",
    "        self.batch_norm = use_batch_norm\n",
    "\n",
    "        self.kernel_size = 4\n",
    "        self.stride_size = 2\n",
    "        self.padding_size = 1\n",
    "\n",
    "        self.encoder = self.block()\n",
    "\n",
    "    def block(self):\n",
    "        layers = OrderedDict()\n",
    "\n",
    "        layers[\"conv\"] = nn.Conv2d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=self.kernel_size,\n",
    "            stride=self.stride_size,\n",
    "            padding=self.padding_size,\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "        if self.leaky_relu:\n",
    "            layers[\"leaky_ReLU\"] = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            layers[\"batch_norm\"] = nn.BatchNorm2d(num_features=self.out_channels)\n",
    "\n",
    "        return nn.Sequential(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return self.encoder(x)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"X should be in the format of tensor\".capitalize())\n",
    "\n",
    "    @staticmethod\n",
    "    def total_params(model):\n",
    "        if isinstance(model, torch.nn.modules.container.Sequential):\n",
    "            return sum(params.numel() for params in model.parameters())\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Model should be in the format of Sequential\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    in_channels = 3\n",
    "    out_channels = 64\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    encoder1 = EncoderBlock(in_channels=in_channels, out_channels=out_channels, use_leaky_relu=True, use_batch_norm=False)\n",
    "    encoder2 = EncoderBlock(in_channels=out_channels, out_channels=out_channels*2, use_leaky_relu=True, use_batch_norm=True) \n",
    "    encoder3 = EncoderBlock(in_channels=out_channels*2, out_channels=out_channels*4, use_leaky_relu=True, use_batch_norm=True) \n",
    "    encoder4 = EncoderBlock(in_channels=out_channels*4, out_channels=out_channels*8, use_leaky_relu=True, use_batch_norm=True) \n",
    "    encoder5 = EncoderBlock(in_channels=out_channels*8, out_channels=out_channels*8, use_leaky_relu=True, use_batch_norm=True) \n",
    "    encoder6 = EncoderBlock(in_channels=out_channels*8, out_channels=out_channels*8, use_leaky_relu=True, use_batch_norm=True) \n",
    "    encoder7 = EncoderBlock(in_channels=out_channels*8, out_channels=out_channels*8, use_leaky_relu=False,use_batch_norm=False)\n",
    "    \n",
    "    for block in [encoder1, encoder2, encoder3, encoder4, encoder5, encoder6, encoder7]:\n",
    "        layers.append(block)\n",
    "    \n",
    "    model = nn.Sequential(*layers)\n",
    "    \n",
    "    assert EncoderBlock.total_params(model=model) == 15342336\n",
    "    \n",
    "    print(model(torch.randn(1, 3, 256, 256)).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels = 512, out_channels = 512, last_layer=False):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.last_layer = last_layer\n",
    "\n",
    "        self.kernel_size = 4\n",
    "        self.stride_size = 2\n",
    "        self.padding_size = 1\n",
    "\n",
    "        self.decoder = self.block()\n",
    "\n",
    "    def block(self):\n",
    "        layers = OrderedDict()\n",
    "\n",
    "        layers[\"convTranspose\"] = nn.ConvTranspose2d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=self.kernel_size,\n",
    "            stride=self.stride_size,\n",
    "            padding=self.padding_size,\n",
    "            bias=False\n",
    "        )\n",
    "        if self.last_layer:\n",
    "            pass\n",
    "        else:\n",
    "            layers[\"ReLU\"] = nn.ReLU(inplace=True)\n",
    "            layers[\"batch_norm\"] = nn.BatchNorm2d(num_features=self.out_channels)\n",
    "\n",
    "        return nn.Sequential(layers)\n",
    "\n",
    "    def forward(self, x, skip_info=None):\n",
    "        if isinstance(x, torch.Tensor) and isinstance(skip_info, torch.Tensor):\n",
    "            x = self.decoder(x)\n",
    "            return torch.cat((x, skip_info), dim=1)\n",
    "        \n",
    "        else:\n",
    "            if isinstance(x, torch.Tensor) and skip_info is None:\n",
    "                return self.decoder(x)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def total_params(model):\n",
    "        if isinstance(model, torch.nn.modules.container.Sequential):\n",
    "            return sum(params.numel() for params in model.parameters())\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Model should be in the format of Sequential\".capitalize())\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    in_channels = 512\n",
    "    out_channels = 512\n",
    "    \n",
    "    model = DecoderBlock(in_channels=in_channels, out_channels=out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels = 3):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = 64\n",
    "\n",
    "        self.encoder1 = EncoderBlock(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            use_leaky_relu=True,\n",
    "            use_batch_norm=False,\n",
    "        )\n",
    "        self.encoder2 = EncoderBlock(\n",
    "            in_channels=self.out_channels,\n",
    "            out_channels=self.out_channels * 2,\n",
    "            use_leaky_relu=True,\n",
    "            use_batch_norm=True,\n",
    "        )\n",
    "        self.encoder3 = EncoderBlock(\n",
    "            in_channels=self.out_channels * 2,\n",
    "            out_channels=self.out_channels * 4,\n",
    "            use_leaky_relu=True,\n",
    "            use_batch_norm=True,\n",
    "        )\n",
    "        self.encoder4 = EncoderBlock(\n",
    "            in_channels=self.out_channels * 4,\n",
    "            out_channels=self.out_channels * 8,\n",
    "            use_leaky_relu=True,\n",
    "            use_batch_norm=True,\n",
    "        )\n",
    "        self.encoder5 = EncoderBlock(\n",
    "            in_channels=self.out_channels * 8,\n",
    "            out_channels=self.out_channels * 8,\n",
    "            use_leaky_relu=True,\n",
    "            use_batch_norm=True,\n",
    "        )\n",
    "        self.encoder6 = EncoderBlock(\n",
    "            in_channels=self.out_channels * 8,\n",
    "            out_channels=self.out_channels * 8,\n",
    "            use_leaky_relu=True,\n",
    "            use_batch_norm=True,\n",
    "        )\n",
    "        self.encoder7 = EncoderBlock(\n",
    "            in_channels=self.out_channels * 8,\n",
    "            out_channels=self.out_channels * 8,\n",
    "            use_leaky_relu=False,\n",
    "            use_batch_norm=False,\n",
    "        )\n",
    "\n",
    "        self.decoder1 = DecoderBlock(\n",
    "            in_channels=self.out_channels * 8, out_channels=self.out_channels * 8\n",
    "        )\n",
    "        self.decoder2 = DecoderBlock(\n",
    "            in_channels=self.out_channels * 8 * 2, out_channels=self.out_channels * 8\n",
    "        )\n",
    "        self.decoder3 = DecoderBlock(\n",
    "            in_channels=self.out_channels * 8 * 2, out_channels=self.out_channels * 8\n",
    "        )\n",
    "        self.decoder4 = DecoderBlock(\n",
    "            in_channels=self.out_channels * 8 * 2, out_channels=self.out_channels * 4\n",
    "        )\n",
    "        self.decoder5 = DecoderBlock(\n",
    "            in_channels=self.out_channels * 4 * 2, out_channels=self.out_channels * 2\n",
    "        )\n",
    "        self.decoder6 = DecoderBlock(\n",
    "            in_channels=self.out_channels * 2 * 2, out_channels=self.out_channels\n",
    "        )\n",
    "        self.decoder7 = DecoderBlock(\n",
    "            in_channels=self.out_channels * 2,\n",
    "            out_channels=in_channels,\n",
    "            last_layer=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            encoder1 = self.encoder1(x)\n",
    "            encoder2 = self.encoder2(encoder1)\n",
    "            encoder3 = self.encoder3(encoder2)\n",
    "            encoder4 = self.encoder4(encoder3)\n",
    "            encoder5 = self.encoder5(encoder4)\n",
    "            encoder6 = self.encoder6(encoder5)\n",
    "            encoder7 = self.encoder7(encoder6)\n",
    "\n",
    "            decoder1 = self.decoder1(encoder7, encoder6)\n",
    "            decoder2 = self.decoder2(decoder1, encoder5)\n",
    "            decoder3 = self.decoder3(decoder2, encoder4)\n",
    "            decoder4 = self.decoder4(decoder3, encoder3)\n",
    "            decoder5 = self.decoder5(decoder4, encoder2)\n",
    "            decoder6 = self.decoder6(decoder5, encoder1)\n",
    "            output = self.decoder7(decoder6)\n",
    "\n",
    "            return output\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"X should be in the format of tensor\".capitalize())\n",
    "        \n",
    "    @staticmethod\n",
    "    def total_params(model):\n",
    "        if isinstance(model, Generator):\n",
    "            return sum(p.numel() for p in model.parameters())\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Model should be in the Generator\".capitalize()) \n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    config_files = config()\n",
    "    files_path = validate_path(config_files[\"path\"][\"files_path\"])\n",
    "    \n",
    "    in_channels = 3\n",
    "    \n",
    "    netG = Generator(in_channels=in_channels)\n",
    "    \n",
    "    print(summary(model=netG, input_size=(3, 256, 256)))\n",
    "    \n",
    "    draw_graph(model=netG, input_data=torch.randn(1, 3, 256, 256)).visual_graph.render(\n",
    "        filename=os.path.join(files_path, \"netG\"), format=\"jpeg\"\n",
    "    )\n",
    "    \n",
    "    assert netG(torch.randn(1, 3, 256, 256)).size() == (1, 3, 256, 256)\n",
    "    \n",
    "    assert netG.total_params(netG) == 41828992"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorBlock(nn.Module):\n",
    "    def __init__(self, in_channels = 3, out_channels = 64, kernel_size = 4, stride_size = 2, padding_size = 1, last_layer = False):\n",
    "        super(DiscriminatorBlock, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.last_layer = last_layer\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride_size = stride_size\n",
    "        self.padding_size = padding_size\n",
    "\n",
    "        self.discriminator_block = self.block()\n",
    "\n",
    "    def block(self):\n",
    "        layers = OrderedDict()\n",
    "\n",
    "        layers[\"conv\"] = nn.Conv2d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=self.kernel_size,\n",
    "            stride=self.stride_size,\n",
    "            padding=self.padding_size\n",
    "        )\n",
    "\n",
    "        if self.last_layer:\n",
    "            layers[\"tanh\"] = nn.Tanh()\n",
    "\n",
    "        else:\n",
    "            layers[\"leaky_ReLU\"] = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "            layers[\"batch_norm\"] = nn.BatchNorm2d(num_features=self.out_channels)\n",
    "\n",
    "        return nn.Sequential(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return self.discriminator_block(x)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Input should be a tensor\".capitalize())\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def total_params(model):\n",
    "        if isinstance(model, torch.nn.modules.container.Sequential):\n",
    "            return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        else:\n",
    "            raise ValueError(\"Model should be a Sequential model\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    in_channels = 3\n",
    "    out_channels = 64\n",
    "    \n",
    "    layers = []\n",
    "    \n",
    "    for _ in range(3):\n",
    "        layers.append(\n",
    "            DiscriminatorBlock(\n",
    "                in_channels=in_channels, out_channels=out_channels\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        in_channels = out_channels\n",
    "        out_channels *= 2\n",
    "    \n",
    "    for idx in range(2):\n",
    "        layers.append(\n",
    "            DiscriminatorBlock(\n",
    "                in_channels=in_channels,\n",
    "                out_channels= 1 if (idx == 1) else out_channels,\n",
    "                stride_size=1,\n",
    "                last_layer=(idx == 1)\n",
    "            )\n",
    "        )\n",
    "        in_channels = out_channels\n",
    "        out_channels *= 2\n",
    "        \n",
    "    model = nn.Sequential(*layers)\n",
    "    \n",
    "    assert DiscriminatorBlock.total_params(model=model) == 2766657\n",
    "    assert model(torch.randn(1, 3, 256, 256)).size() == (1, 1, 30, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels = 3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.in_channels = 3\n",
    "        self.out_channels = 64\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        for _ in range(3):\n",
    "            layers.append(\n",
    "                DiscriminatorBlock(\n",
    "                    in_channels=self.in_channels, out_channels=self.out_channels\n",
    "                )\n",
    "            )\n",
    "\n",
    "            self.in_channels = self.out_channels\n",
    "            self.out_channels *= 2\n",
    "\n",
    "        for idx in range(2):\n",
    "            layers.append(\n",
    "                DiscriminatorBlock(\n",
    "                    in_channels=self.in_channels,\n",
    "                    out_channels=1 if (idx == 1) else self.out_channels,\n",
    "                    stride_size=1,\n",
    "                    last_layer=(idx == 1),\n",
    "                )\n",
    "            )\n",
    "            self.in_channels = self.out_channels\n",
    "            self.out_channels *= 2\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            x1 = self.model[0](x)\n",
    "            x2 = self.model[1](x1)\n",
    "            x3 = self.model[2](x2)\n",
    "            x4 = self.model[3](x3)\n",
    "            x5 = self.model[4](x4)\n",
    "            \n",
    "            return torch.cat((\n",
    "                x1.view(x1.size(0), -1),\n",
    "                x2.view(x2.size(0), -1),\n",
    "                x3.view(x3.size(0), -1),\n",
    "                x4.view(x4.size(0), -1),\n",
    "                x5.view(x5.size(0), -1)), dim=1\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"X should be in the format of tenor\".capitalize())\n",
    "        \n",
    "    @staticmethod\n",
    "    def total_params(model):\n",
    "        if isinstance(model, Discriminator):\n",
    "            return sum(params.numel() for params in model.parameters() if params.requires_grad)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Model should be in the format of Discriminator\".capitalize())\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    config_files = config()\n",
    "    files_path = validate_path(path=config_files[\"path\"][\"files_path\"])\n",
    "    \n",
    "    in_channels = 3\n",
    "    \n",
    "    netD = Discriminator(in_channels=in_channels)\n",
    "    \n",
    "    print(summary(model=netD, input_size=(in_channels, 256, 256)))\n",
    "    \n",
    "    draw_graph(model=netD, input_data=torch.randn(1, in_channels, 256, 256)).visual_graph.render(\n",
    "        filename=os.path.join(files_path, \"netD\"), format=\"jpeg\"\n",
    "    )\n",
    "    \n",
    "    assert Discriminator.total_params(model=netD) == 2766657\n",
    "    \n",
    "    assert netD(torch.randn(1,3,256,256)).size() == (1, 2327940)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1Loss(nn.Module):\n",
    "    def __init__(self, reduction = \"mean\"):\n",
    "        super(L1Loss, self).__init__()\n",
    "        \n",
    "        self.loss_name = \"L1Loss\".title()\n",
    "        \n",
    "        self.reduction = reduction\n",
    "    \n",
    "        self.l1_loss = nn.L1Loss(reduction=self.reduction)\n",
    "        \n",
    "    def forward(self, actual, predicted):\n",
    "        if isinstance(actual, torch.Tensor) and isinstance(predicted, torch.Tensor):\n",
    "            return self.l1_loss(actual, predicted)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Actual and Predicted should be in the format of tensor\".capitalize())\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    l1loss = L1Loss(reduction=\"mean\")\n",
    "    \n",
    "    actual = torch.tensor([1.0, 0.0, 1.0])\n",
    "    predicted = torch.tensor([1.0, 0.0, 1.0])\n",
    "    \n",
    "    print(\"Total loss is # {}\".format(l1loss(actual, predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=0.001):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "        self.loss_name = \"Dice Loss\".capitalize()\n",
    "\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, predicted, actual):\n",
    "        if isinstance(predicted, torch.Tensor) and isinstance(actual, torch.Tensor):\n",
    "            actual = actual.view(-1)\n",
    "            predicted = predicted.view(-1)\n",
    "\n",
    "        intersection = (predicted * actual).sum()\n",
    "        dice = (2.0 * intersection + self.smooth) / (\n",
    "            predicted.sum() + actual.sum() + self.smooth\n",
    "        )\n",
    "\n",
    "        return 1.0 - dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### helpers method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataloader():\n",
    "    config_files = config()\n",
    "    processed_path = config_files[\"path\"][\"processed_path\"]\n",
    "    processed_path = validate_path(path=processed_path)\n",
    "\n",
    "    train_dataloader = load(\n",
    "        filename=os.path.join(processed_path, \"train_dataloader.pkl\")\n",
    "    )\n",
    "    test_dataloader = load(filename=os.path.join(processed_path, \"test_dataloader.pkl\"))\n",
    "\n",
    "    return {\"train_dataloader\": train_dataloader, \"test_dataloader\": test_dataloader}\n",
    "\n",
    "\n",
    "def helpers(**kwargs):\n",
    "    channels = kwargs[\"channels\"]\n",
    "    lr = kwargs[\"lr\"]\n",
    "    adam = kwargs[\"adam\"]\n",
    "    SGD = kwargs[\"SGD\"]\n",
    "\n",
    "    netG = Generator(in_channels=channels)\n",
    "    netD = Discriminator(in_channels=channels)\n",
    "\n",
    "    if adam:\n",
    "        optimizerG = optim.Adam(params=netG.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "        optimizerD = optim.Adam(params=netD.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "    elif SGD:\n",
    "        optimizerG = optim.SGD(params=netG.parameters(), lr=lr, momentum=0.9)\n",
    "        optimizerD = optim.SGD(params=netD.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    l1loss = L1Loss(reduction=\"mean\")\n",
    "    diceloss = DiceLoss(smooth=0.001)\n",
    "    dataloader = load_dataloader()\n",
    "\n",
    "    return {\n",
    "        \"netG\": netG,\n",
    "        \"netD\": netD,\n",
    "        \"optimizerG\": optimizerG,\n",
    "        \"optimizerD\": optimizerD,\n",
    "        \"l1loss\": l1loss,\n",
    "        \"diceloss\": diceloss,\n",
    "        \"train_dataloader\": dataloader[\"train_dataloader\"],\n",
    "        \"test_dataloader\": dataloader[\"test_dataloader\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from .helper import helpers\n",
    "from .utils import weight_init, device_init, config, validate_path, dump, load\n",
    "from .generator import Generator\n",
    "from .discriminator import Discriminator\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels=3,\n",
    "        lr=0.0002,\n",
    "        epochs=100,\n",
    "        adam=True,\n",
    "        SGD=False,\n",
    "        device=\"cuda\",\n",
    "        beta1=0.5,\n",
    "        beta2=0.999,\n",
    "        momentum=0.90,\n",
    "        smooth=0.001,\n",
    "        step_size=10,\n",
    "        gamma=0.5,\n",
    "        lr_scheduler=False,\n",
    "        is_display=True,\n",
    "        is_weight_init=True,\n",
    "    ):\n",
    "\n",
    "        self.channels = channels\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.adam = adam\n",
    "        self.SGD = SGD\n",
    "        self.device = device\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.momentum = momentum\n",
    "        self.smooth = smooth\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.is_display = is_display\n",
    "        self.is_weight_init = is_weight_init\n",
    "\n",
    "        self.device = device_init(device=self.device)\n",
    "\n",
    "        self.init = helpers(\n",
    "            channels=self.channels,\n",
    "            lr=self.lr,\n",
    "            adam=self.adam,\n",
    "            SGD=self.SGD,\n",
    "            beta1=self.beta1,\n",
    "            beta2=self.beta2,\n",
    "            momentum=self.momentum,\n",
    "            smooth=self.smooth,\n",
    "        )\n",
    "\n",
    "        self.netG = self.init[\"netG\"]\n",
    "        self.netD = self.init[\"netD\"]\n",
    "\n",
    "        self.optimizerG = self.init[\"optimizerG\"]\n",
    "        self.optimizerD = self.init[\"optimizerD\"]\n",
    "\n",
    "        self.l1loss = self.init[\"l1loss\"]\n",
    "        self.diceloss = self.init[\"diceloss\"]\n",
    "\n",
    "        self.train_dataloader = self.init[\"train_dataloader\"]\n",
    "        self.test_dataloader = self.init[\"test_dataloader\"]\n",
    "\n",
    "        self.netG.to(self.device)\n",
    "        self.netD.to(self.device)\n",
    "\n",
    "        if self.is_weight_init:\n",
    "            self.netG.apply(weight_init)\n",
    "            self.netD.apply(weight_init)\n",
    "\n",
    "        if self.lr_scheduler:\n",
    "            self.schedulerG = StepLR(\n",
    "                optimizer=self.optimizerG, step_size=self.step_size, gamma=self.gamma\n",
    "            )\n",
    "            self.schedulerD = StepLR(\n",
    "                optimizer=self.optimizerD, step_size=self.step_size, gamma=self.gamma\n",
    "            )\n",
    "\n",
    "        self.loss = float(\"inf\")\n",
    "        self.total_netG_loss = []\n",
    "        self.total_netD_loss = []\n",
    "        self.history = {\"netG_loss\": [], \"netD_loss\": []}\n",
    "\n",
    "        self.train_images_path = validate_path(\n",
    "            path=config()[\"path\"][\"train_images_path\"]\n",
    "        )\n",
    "        self.train_model = validate_path(path=config()[\"path\"][\"train_model_path\"])\n",
    "        self.best_model = validate_path(path=config()[\"path\"][\"best_model_path\"])\n",
    "        self.metrics_path = validate_path(path=config()[\"path\"][\"metrics_path\"])\n",
    "\n",
    "    def l1_loss(self, model):\n",
    "        if isinstance(model, Generator):\n",
    "            return (torch.norm(params, 1) for params in model.parameters()).mean()\n",
    "        else:\n",
    "            raise ValueError(\"The model is not a Generator\".capitalize())\n",
    "\n",
    "    def l2_loss(self, model):\n",
    "        if isinstance(model, Generator):\n",
    "            return (torch.norm(params, 2) for params in model.parameters()).mean()\n",
    "        else:\n",
    "            raise ValueError(\"The model is not a Generator\".capitalize())\n",
    "\n",
    "    def elastic_loss(self, model):\n",
    "        if isinstance(model, Generator):\n",
    "            l1 = self.l1_loss(model=model)\n",
    "            l2 = self.l2_loss(model=model)\n",
    "\n",
    "            return l1 + l2\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"The model is not a Generator\".capitalize())\n",
    "\n",
    "    def saved_model_checkpoints(self, **kwargs):\n",
    "        torch.save(\n",
    "            self.netG.state_dict(),\n",
    "            os.path.join(self.train_model, \"netG{}.pth\".format(kwargs[\"epoch\"])),\n",
    "        )\n",
    "\n",
    "        if self.loss > kwargs[\"netG_loss\"]:\n",
    "            self.loss = kwargs[\"netG_loss\"]\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"netG\": self.netG.state_dict(),\n",
    "                    \"netD\": self.netD.state_dict(),\n",
    "                    \"loss\": kwargs[\"netG_loss\"],\n",
    "                },\n",
    "                os.path.join(self.best_model, \"best_model.pth\"),\n",
    "            )\n",
    "\n",
    "    def saved_training_images(self, **kwargs):\n",
    "        save_image(\n",
    "            kwargs[\"predicted_mask\"],\n",
    "            os.path.join(\n",
    "                self.train_images_path, \"image{}.png\".format(kwargs[\"epoch\"] + 1)\n",
    "            ),\n",
    "            nrow=32,\n",
    "            normalize=True,\n",
    "        )\n",
    "\n",
    "    def show_progress(self, **kwargs):\n",
    "        if self.is_display:\n",
    "            print(\n",
    "                \"Epochs - [{}/{}] - netG_loss: {:.4f} - netD_loss: {:.4f}\".format(\n",
    "                    kwargs[\"epoch\"],\n",
    "                    kwargs[\"epochs\"],\n",
    "                    kwargs[\"netG_loss\"],\n",
    "                    kwargs[\"netD_loss\"],\n",
    "                )\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            print(\n",
    "                \"Epochs - [{}/{}] is completed\".format(\n",
    "                    kwargs[\"epoch\"], kwargs[\"epochs\"]\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def update_train_netG(self, **kwargs):\n",
    "        self.optimizerG.zero_grad()\n",
    "\n",
    "        images = kwargs[\"image\"]\n",
    "        masks = kwargs[\"mask\"]\n",
    "\n",
    "        fake_masks = self.netG(images)\n",
    "        fake_masks = torch.sigmoid(fake_masks)\n",
    "\n",
    "        fakeB = images * fake_masks\n",
    "        realA = images * masks\n",
    "\n",
    "        fake_masks_loss = self.diceloss(fake_masks, masks)\n",
    "\n",
    "        real_predict = self.netD(realA)\n",
    "        fake_predict = self.netD(fakeB.detach())\n",
    "\n",
    "        multiscale_loss = self.l1loss(real_predict, fake_predict)\n",
    "\n",
    "        lossG = 0.1 * fake_masks_loss + multiscale_loss\n",
    "\n",
    "        lossG.backward()\n",
    "        self.optimizerG.step()\n",
    "\n",
    "        return lossG.item()\n",
    "\n",
    "    def update_train_netD(self, **kwargs):\n",
    "        self.optimizerD.zero_grad()\n",
    "\n",
    "        images = kwargs[\"image\"]\n",
    "        masks = kwargs[\"mask\"]\n",
    "\n",
    "        fake_masks = self.netG(images)\n",
    "        fake_masks = torch.sigmoid(fake_masks)\n",
    "\n",
    "        realA = images * masks\n",
    "        fakeB = images * fake_masks\n",
    "\n",
    "        real_predict = self.netD(realA)\n",
    "        fake_predict = self.netD(fakeB.detach())\n",
    "\n",
    "        lossD = 1 - self.l1loss(real_predict, fake_predict)\n",
    "\n",
    "        lossD.backward()\n",
    "        self.optimizerD.step()\n",
    "\n",
    "        for params in self.netD.parameters():\n",
    "            params.data.clamp_(-0.01, 0.01)\n",
    "\n",
    "        return lossD.item()\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "            netG_loss = []\n",
    "            netD_loss = []\n",
    "\n",
    "            for index, (image, mask) in enumerate(self.train_dataloader):\n",
    "                try:\n",
    "                    image = image.to(self.device)\n",
    "                    mask = mask.to(self.device)\n",
    "\n",
    "                    netD_loss.append(self.update_train_netD(image=image, mask=mask))\n",
    "                    netG_loss.append(self.update_train_netG(image=image, mask=mask))\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        f\"Error during training at epoch {epoch + 1}, batch {index}: {e}\"\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "            try:\n",
    "                self.show_progress(\n",
    "                    epoch=epoch + 1,\n",
    "                    epochs=self.epochs,\n",
    "                    netD_loss=np.mean(netD_loss),\n",
    "                    netG_loss=np.mean(netG_loss),\n",
    "                )\n",
    "\n",
    "                image, mask = next(iter(self.test_dataloader))\n",
    "                image = image.to(self.device)\n",
    "                predicted_mask = self.netG(image)\n",
    "\n",
    "                self.saved_training_images(\n",
    "                    image=image, mask=mask, predicted_mask=predicted_mask, epoch=epoch\n",
    "                )\n",
    "\n",
    "                self.saved_model_checkpoints(\n",
    "                    netG_loss=np.mean(netG_loss),\n",
    "                    epoch=epoch + 1,\n",
    "                )\n",
    "\n",
    "                if self.lr_scheduler:\n",
    "                    self.schedulerD.step()\n",
    "                    self.schedulerG.step()\n",
    "\n",
    "                self.total_netG_loss.append(np.mean(netG_loss))\n",
    "                self.total_netD_loss.append(np.mean(netD_loss))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error during post-epoch processing at epoch {epoch + 1}: {e}\")\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            self.history[\"netG_loss\"].append(self.total_netG_loss)\n",
    "            self.history[\"netD_loss\"].append(self.total_netD_loss)\n",
    "\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"netG_loss\": self.total_netG_loss,\n",
    "                    \"netD_loss\": self.total_netD_loss,\n",
    "                }\n",
    "            ).to_csv(os.path.join(self.metrics_path, \"model_history.csv\"))\n",
    "\n",
    "            for filename, value in [\n",
    "                (\"history.pkl\", self.history),\n",
    "                (\"netG_loss.pkl\", self.total_netG_loss),\n",
    "                (\"netD_loss.pkl\", self.total_netD_loss),\n",
    "            ]:\n",
    "                dump(value=value, filename=os.path.join(self.metrics_path, filename))\n",
    "\n",
    "            print(\n",
    "                \"Saved the model history in a csv file in {}\\nSaved the model history in pickle format in {}\".format(\n",
    "                    self.metrics_path, self.metrics_path\n",
    "                )\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during post-training processing: {e}\")\n",
    "            return\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_history():\n",
    "        metrics_path = config()[\"path\"][\"metrics_path\"]\n",
    "        metrics_path = validate_path(path=metrics_path)\n",
    "\n",
    "        plt.figure(figsize=(20, 20))\n",
    "\n",
    "        history = load(filename=os.path.join(metrics_path, \"history.pkl\"))\n",
    "\n",
    "        for index, (title, loss) in enumerate(\n",
    "            [\n",
    "                (\"netG_loss\", history[\"netG_loss\"]),\n",
    "                (\"netD_loss\", history[\"netD_loss\"]),\n",
    "            ]\n",
    "        ):\n",
    "            plt.subplot(2 * 1, 2 * 2, 2 * index + (index + 1))\n",
    "\n",
    "            plt.plot(loss[0], label=title)\n",
    "            plt.title(f\"{title}\")\n",
    "            plt.xlabel(\"Epochs\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_image(os.path.join(metrics_path, \"model_history.jpeg\"))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from .utils import validate_path, config, device_init, load\n",
    "from .generator import Generator\n",
    "\n",
    "\n",
    "class TestModel:\n",
    "    def __init__(self, device=\"cuda\"):\n",
    "        try:\n",
    "            self.device = device_init(device=device)\n",
    "            self.test_dataloader = validate_path(config()[\"path\"][\"processed_path\"])\n",
    "            self.best_model = validate_path(\n",
    "                path=os.path.join(config()[\"path\"][\"best_model_path\"])\n",
    "            )\n",
    "            self.test_image_path = validate_path(\n",
    "                path=config()[\"path\"][\"test_image_path\"]\n",
    "            )\n",
    "            self.netG = Generator().to(self.device)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during initialization: {e}\")\n",
    "            raise\n",
    "\n",
    "    def load_dataset(self):\n",
    "        try:\n",
    "            return load(\n",
    "                filename=os.path.join(self.test_dataloader, \"test_dataloader.pkl\")\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset: {e}\")\n",
    "            raise\n",
    "\n",
    "    def load_best_model(self):\n",
    "        try:\n",
    "            self.netG.load_state_dict(\n",
    "                torch.load(os.path.join(self.best_model, \"best_model.pth\"))[\"netG\"]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading the best model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def plot(self):\n",
    "        try:\n",
    "            plt.figure(figsize=(20, 10))\n",
    "\n",
    "            image, mask = next(iter(self.load_dataset()))\n",
    "            predict = self.netG(image.to(self.device))\n",
    "\n",
    "            size = image.size(0)\n",
    "            num_row = size // 2\n",
    "            num_columns = size // num_row\n",
    "\n",
    "            for index, image in enumerate(predict):\n",
    "                plt.subplot(2 * num_row, 2 * num_columns, 2 * index + 1)\n",
    "                image = image.squeeze().cpu().permute(1, 2, 0).detach().numpy()\n",
    "                masks = mask[index].squeeze().cpu().permute(1, 2, 0).detach().numpy()\n",
    "\n",
    "                image = (image - image.min()) / (image.max() - image.min())\n",
    "                masks = (masks - masks.min()) / (masks.max() - masks.min())\n",
    "\n",
    "                plt.imshow(image, cmap=\"gray\")\n",
    "                plt.axis(\"off\")\n",
    "                plt.title(\"Generated Image\")\n",
    "\n",
    "                plt.subplot(2 * num_row, 2 * num_columns, 2 * index + 2)\n",
    "                plt.imshow(masks)\n",
    "                plt.title(\"Mask\")\n",
    "                plt.axis(\"off\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(self.test_image_path, \"test.png\"))\n",
    "            plt.show()\n",
    "\n",
    "            print(\"Test image is saved in the path: \", self.test_image_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during plotting: {e}\")\n",
    "            raise\n",
    "\n",
    "    def test(self):\n",
    "        try:\n",
    "            self.load_best_model()\n",
    "            self.plot()\n",
    "        except Exception as e:\n",
    "            print(f\"Error during testing: {e}\")\n",
    "            raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPSG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
